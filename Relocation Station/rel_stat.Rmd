---
title: 'Relocation Stations'
author: Erich Grennan-Mengore, Natalie Brum
output:
  html_notebook: default
---
###Introduction - Parts of this is subject to change later
This project is based on Erich Grennan-Mengore's exploratory data analysis for the cost of living in 2018 project in Reinaldo Sanchez-Arias's buisness intelligence class during the Fall 2018 Semester. 

// EDIT THIS

In this exploratory Data Analysis, we are investigating the Cost of Living in 2018 dataset, which is composed of Cost of Living for cities around the world 2018. The question I wish to get an answer to, is how do you the determine the best Cities within the continental United States to live in based off its popularity and potential for making the most money? Like any young person I would like to go off to different city to live once I finish up my education. To do that requires a lot of research beforehand to avoid a disaster, such as moving somewhere and ending up homeless or living paycheck to paycheck. Not only that but determining the best cities for that can also determine other types of city sub-groups which could be useful for different age demographics such as parents and retirees who might not want to live a high-rise lifestyle like many my age. This Data Analysis is built on top of the Cost of Living Index by Numbeo.com. How they collected the data was through user inputs and manually collected data from authoritative sources, with this process going about twice per year and the use of many sets of filters to eliminate bad data. The variables being used in the data are all based of New York’s value (100) as a baseline and can be explained as follows:
	
*Cost of Living Index - is a relative indicator of consumer goods prices, including groceries, restaurants, transportation and utilities. This does not include accommodation expense such as rent of mortgage. This index is built based on the best guess of average expenses in a given city for a four-person family. The lower this value the better.*
	
*Rent Index – is an estimation of prices of renting apartments in that city compared to New York. The lower this value the better.*
	
*Groceries Index – is an estimation of grocery prices in the city compared to New York.*
	
*Restaurants Index – is a comparison for prices for meals and drinks in restaurants and bars compared to NYC. The lower this value the better.*

*Cost of Living Plus Rent Index – is a combined estimation of the values that make up cost of Living Index and Rent Index. It is calculated by Numbeo as such:*

    CostOfLivingPlusRent_City = ∑(Price_in_the_city*(cost_of_living_factor + rent_factor))
	
*Local Purchasing Power - shows relative purchasing power in buying goods and services in a given city for the average wage in that city. The higher this value the better. It is calculated by Numbeo as such:*
	
	LocalPurchasingPowerIndex =  (AverageDisposableSalary_City⁄CostOfLivingPlusRent_City)/(〖AverageDisposableSalary_(New York)⁄CostOfLivingPlusRent_(New York) )
	

*Average Disposable Salary – shows the Average Disposable salary that is being made in that city.This is calculated by performing a bit of algebra variables that make up the Local Purchasing Power Index formula, which is:*

	AverageDisposableSalary_City=((LocalPurchasingPower*CostOfLivingPlusRent))/100
  - Note: The value of Average Disposable Salary of New York was dropped due to it being an unknown constant, but not needed to   determine a sort for Average Disposable Salary. 
	
*Latitude- Self-explanatory, shows up lat in data.*
*Longitude – Self-explanatory, shows up as lng in data.*

###Exploratory Data Analysis - Parts of this is subject to change as well.
Loading of All Libraries
```{r}
library(tidyverse)
library(geonames)
library(maps)
library(mapdata)
library(dendextend)
library(forcats)
library(ggrepel)
library(GSODR)
options(geonamesUsername= "egrennanmengore2488", geonamesHost="api.geonames.org")
```

The distance for nearest stations is set to 40KM which is equal to around 50miles
```{r echo=FALSE, message=FALSE}
wxForCityByYr <- function(latitude, longitude, year){
  station <- nearest_stations(latitude, longitude, 40)
  isGT365 <- FALSE
  count <- 0
  while (isGT365 == FALSE){
    count <- count + 1
      res <- suppressWarnings(get_GSOD(years = year, station = station[[count]]))
    if(nrow(res) >= 365){
      isGT365 <- TRUE
      return(res)
    }
  }
}
```

```{r}
testData <- wxForCityByYr(37.77493, -122.41942, 2015)
x<- testData %>% filter(YEARMODA >= "2015-01-01" & YEARMODA <= "2015-02-28") %>% select(TEMP)
str(testData)
head(x)
```

```{r echo=FALSE, message=FALSE}

```

```{r echo=FALSE, message=FALSE}
winter <- vector("double", length = 4)
spring <- vector("double", length = 4)
summer <- vector("double", length = 4)
fall <- vector("double", length = 4)
for(row in 1:nrow(usCol)){
  lat <- usCol[row, "lat"]
  lng <- usCol[row, "lng"]
  res <- wxForCityByYr(lat, lng, 2015)
  for(i in 1:4){
    if (i==1){
      winter <- res %>% filter((YEARMODA >= "2014-12-01" & YEARMODA <= "2015-02-28") %>% select(TEMP))
      winter[i] <- mean(winter$TEMP)
    }
    if (i==2){
      winter <- res %>% filter((YEARMODA >= "2015-12-01" & YEARMODA <= "2016-02-28") %>% select(TEMP))
      winter[i]<- mean(winter$TEMP)
    }
    if (i==3){
      winter <- res %>% filter((YEARMODA >= "2016-12-01" & YEARMODA <= "2017-02-28") %>% select(TEMP))
      winter[i] <- mean(winter$TEMP)
    }
    if (i==4){
      winter <- res %>% filter((YEARMODA >= "2017-12-01" & YEARMODA <= "2018-02-28") %>% select(TEMP))
      winter[i] <- mean(winter$TEMP)
    }
  }
  winter2015 <- res %>% filter(YEARMODA >= "2014-12-01" & YEARMODA <= "2015-02-28") %>% select(TEMP)
  usCol[row, "Winter.Mean"] <- mean(winter2015$TEMP)
  spring2015 <- res %>% filter(YEARMODA >= "2015-03-01" & YEARMODA <= "2015-05-31") %>% select(TEMP)
  usCol[row, "Spring.Mean"] <- mean(spring2015$TEMP)
  summer2015 <- res %>% filter(YEARMODA >= "2015-06-01" & YEARMODA <= "2015-08-31") %>% select(TEMP)
  usCol[row, "Summer.Mean"] <- mean(summer2015$TEMP)
  fall2015 <- res %>% filter(YEARMODA >= "2015-09-01" & YEARMODA <= "2015-11-30") %>% select(TEMP)
  usCol[row, "Fall.Mean"] <- mean(fall2015$TEMP)
}
```
```{r}
dataList <- vector("double", length = 4)
dataList[1] <- 2.6
dataList[2] <- 4.5
dataList[3] <- 3.5
dataList[4] <- 1.2
for(i in 1:4){
    if (i==1){
      winter_res <- res %>% filter((YEARMODA >= "2014-12-01" & YEARMODA <= "2015-02-28") %>% select(TEMP))
      winter[i] <- mean(winter$TEMP)
    }
    if (i==2){
      winter_res <- res %>% filter((YEARMODA >= "2015-12-01" & YEARMODA <= "2016-02-28") %>% select(TEMP))
      winter[i]<- mean(winter$TEMP)
    }
    if (i==3){
      winter_res <- res %>% filter((YEARMODA >= "2016-12-01" & YEARMODA <= "2017-02-28") %>% select(TEMP))
      winter[i] <- mean(winter$TEMP)
    }
    if (i==4){
      winter_res <- res %>% filter((YEARMODA >= "2017-12-01" & YEARMODA <= "2018-02-28") %>% select(TEMP))
      winter[i] <- mean(winter$TEMP)
    }
  }
head(dataList)
mean(dataList)
```

```{r}
head(usCol)
```

```{r}
dataList = list()
for (year in 2015:2018) {
  x <- get_GSOD(year, country = "United States" )
  data <- x %>% select(c(6,8:11,15:17,19,21,29,33,35,37,39:45,48))
  data <- data.frame(data)
  dataList[[year - 2014]] <- data
}
gsod08_18 <- bind_rows(dataList)
```



Here we start off by performing some data pre-processing and fetching data points need to help with visualizations of this data. A description of how all the pre-processing works will be described within the methods section of this document.
```{r}
col <- read.csv("https://raw.githubusercontent.com/reisanar/datasets/master/cost-of-living-2018.csv")
usCol<- col %>% 
  filter(grepl("United States", City)) %>%
  separate(City, c("City","State"), sep = "[,]")
usCol <- usCol %>% filter(City != "Honolulu" & City != "Anchorage")
usCol$Avg.Disp.Sal.Index <- NA
usCol$lat <- NA
usCol$lng <- NA
for(row in 1:nrow(usCol)){
  x <- usCol[row, "Local.Purchasing.Power.Index"]
  y <- usCol[row, "Cost.of.Living.Plus.Rent.Index"]
  res <- (x*y)/100
  usCol[row, "Avg.Disp.Sal.Index"] <- res
}
for(row in 1:nrow(usCol)){
  x <- usCol[row, "City"]
  y <- usCol[row, "State"]
  res <- GNsearch(name = x, adminCode1 = y, country="US", maxRows = 1)
  usCol[row, "lat"] <- res$lat
  usCol[row, "lng"] <- res$lng
}
usCol$Avg.Disp.Sal.Index <- as.double(usCol$Avg.Disp.Sal.Index)
usCol$lat <- as.double(usCol$lat)
usCol$lng <- as.double(usCol$lng)
usCol$State <- as.factor(usCol$State)
usa <- map_data("state")
rm(col, res, x, y, row)
```

```{r}
write_csv(usCol, "usCol.csv")
```

```{r}
usCol <- read.csv("usCol.csv")
```


```{r}
set.seed(20)
d_usCol <- dist(usCol[, c("Cost.of.Living.Index", "Rent.Index", "Avg.Disp.Sal.Index")])
hc_usCol <- hclust(d_usCol, method = "complete")
m1 <- color_labels(as.dendrogram(hc_usCol))
plot(set(m1, "labels_cex", 0.7))
comp2 <- cutree(hc_usCol, 5)
usCol$City.Category <- as.factor(comp2)
```
Above we first set a seed so that whenever run the code above we get the same results.The distance vector is then cut based of the distance of Cost.of.Living.Index, Rent.Index, and Avg.Dis.Sal.Index. These variables were choosen because they are used to make up two of our most important elements. Then we input the distance vector into Hclustering algorithm using the "complete" method. From the denrogram above I believe that it would probably best to cut 5 groups from the tree for use as factors with data.

```{r}
usCol <- mutate(usCol, City.Category = fct_recode(City.Category,
                            "Highest Popularity,\nLow Cash Potential" = "1",
                             "High Popularity,\nLow Cash Potential" = "2",
                             "High Popularity,\nHigh Cash Potential" = "3",
                             "Lowest-Low Popularity,\nLow-Avg Cash Potential" = "4",
                             "Low-Avg Popularity,\nHighest Cash Potential" = "5"
                             ))

```
## Results
Below is the Loop that takes each category of cities and maps them based of thier latitude and longitude to the appropriate point on the map, this is the results of our clustering analysis. 
Based on the maps below it is most likely that the Cities that fit or might fit my interest witll most likely be categoriy "High Popularity, High Cash Potential", since it provides the best grounds for Local Purchasing Power and are still relatively big cities that have a lot for someone of my age. It is also interesting to see that some of the really popular cities start having diminishing returns in terms of pay, which could be due to being too desirable raising rates of expenses. Either way these categorizations can be useful depending on the persons place in life and intentions in terms of cities they wish to live in. An example of how this can be useful is the "Low-Avg Popularty, High Cash Potential" category, as this could be the perfect category if I were older with a family looking to settle down. 

```{r}
for(i in levels(usCol$City.Category)){
  s1 <- usCol %>% filter(City.Category == i)
  p1 <- ggplot(data = s1, aes(x = lng, y = lat)) + labs(title = s1$City.Category) + geom_polygon(data = usa, aes(x = long, y = lat, group = group, fill = region), color = "white") + coord_fixed(1.3) + guides(fill=FALSE) + geom_point(size = 1) + scale_color_manual(values = c("black"))
  print(p1 + geom_text_repel(aes(label = City) , size = 2.5))
}
```


Although some insights were gainined into cities, I beleive more research should be done to make solid claims. Gathering of more types of data should be done as well, since salary did not full explain the cost of living even if it did explain a part of it.
